# 1. Import Required Libraries

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import random
import numpy as np

torch.__version__

torchvision.__version__

# 2. Set-Up Device-Agnostic Code

device = "cuda" if torch.cuda.is_available() else "cpu"

!nvidia-smi

torch.cuda.is_available()

# 3. Set the seed

torch.manual_seed(42)
torch.cuda.manual_seed(42)
np.random.seed(42)
random.seed(42)

# 4. Setting the Hyperparameters

BATCH_SIZE = 128
EPOCHS = 10
LEARNING_RATE = 3e-4
PATCH_SIZE = 4
NUM_CLASSES = 10
IMAGE_SIZE = 32
CHANNELS = 3
EMBEDDING_DIM = 256
NUM_HEADS = 8
DEPTH = 6
MLP_DIM = 512
DROP_RAte = 0.1


# 5. Define Image Transformation

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5), (0.5))
])

# 6. Getting Datasets

train_dataset = datasets.CIFAR10(root="data",
                                 train=True,
                                 download=True,
                                 transform=transform)

test_dataset = datasets.CIFAR10(root="data",
                                train=False,
                                download=True,
                                transform=transform)

train_dataset

test_dataset

# 7. Converting our dataset into dataloaders

train_loader = DataLoader(dataset=train_dataset,
                          batch_size=BATCH_SIZE,
                          shuffle=True)

test_loader = DataLoader(dataset=test_dataset,
                         batch_size=BATCH_SIZE,
                         shuffle=False)

# lets check what we have created
print(f"DataLoader: {train_loader, test_loader}")
print(f"lenght of train_Loader: {len(train_loader)} batches of {BATCH_SIZE}...")
print(f"lenght of test_Loader: {len(test_loader)} batches of {BATCH_SIZE}...")

# 8. Building Visio Transformer from sratch

class PatchEmbedding(nn.Module):
    def __init__(self,
                 imag_size,
                 patch_size,
                 in_channels,
                 embedding_dim):
      super().__init__()
      self.patch_size = patch_size
      self.proj = nn.Conv2d(in_channels=in_channels,
                            out_channels=embedding_dim,
                            kernel_size=patch_size,
                            stride=patch_size)
      num_patches = (imag_size // patch_size) ** 2
      self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))
      self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embedding_dim))

    def forward(self, x:torch.Tensor):
      B = x.size(0)
      x = self.proj(x)
      x = x.flatten(2).transpose(1,2)
      # Expand the cls_token to match the batch size B
      cls_tokens = self.cls_token.expand(B, -1, -1)
      x = torch.cat((cls_tokens, x), dim=1)
      x = x + self.pos_embedding
      return x

class MLP(nn.Module):
  def __init__(self,
               in_features,
               hidden_features,
               drop_rate):
     super().__init__()
     self.fc1 = nn.Linear(in_features=in_features,
                          out_features=hidden_features)
     self.fc2 = nn.Linear(in_features=hidden_features,
                          out_features=in_features)
     self.dropout = nn.Dropout(p=drop_rate)

  def forward(self, x):
    x = self.dropout(F.gelu(self.fc1(x)))
    x = self.dropout(self.fc2(x))
    return x

class TransformerEncoderLayer(nn.Module):
  def __init__(self,
               embedding_dim,
               num_heads,
               mlp_dim,
               drop_rate):
    super().__init__()
    self.norm1 = nn.LayerNorm(embedding_dim)
    self.attn = nn.MultiheadAttention(embedding_dim, num_heads, dropout=drop_rate)
    self.norm2 = nn.LayerNorm(embedding_dim)
    self.mlp = MLP(embedding_dim, mlp_dim, drop_rate)

  def forward(self, x):
    x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]
    x = x + self.mlp(self.norm2(x))
    return x

class VisionTransformer(nn.Module):
  def __init__(self, imag_size, patch_size, in_channels, num_classes, embedding_dim, depth, num_heads, mlp_dim, drop_rate):
    super().__init__()
    self.patch_embedding = PatchEmbedding(imag_size, patch_size, in_channels, embedding_dim)
    self.encoder = nn.Sequential(*[
        TransformerEncoderLayer(embedding_dim, num_heads, mlp_dim, drop_rate)
        for _ in range(depth)
    ])
    self.norm = nn.LayerNorm(embedding_dim)
    self.head = nn.Linear(embedding_dim, num_classes)

  def forward(self, x):
    x = self.patch_embedding(x)
    x = self.encoder(x)
    x = self.norm(x)
    cls_token = x[:, 0]
    return self.head(cls_token)

# Instantiate Model
model = VisionTransformer(
    IMAGE_SIZE, PATCH_SIZE, CHANNELS, NUM_CLASSES,
    EMBEDDING_DIM, DEPTH, NUM_HEADS, MLP_DIM, DROP_RAte
).to(device)

model

# 9. Defining a Loss Function and an optimizer

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params=model.parameters(),
                             lr=LEARNING_RATE)

criterion

optimizer

# 10. Defining a Training Loop Function

def train(model, loader, criterion, optimizer):
  # Set the mode of the model into the training
  model.train()

  total_loss ,correct = 0,0
  for x, y in loader:
    # Moving (Sending) our data in to the target device
    x, y = x.to(device), y.to(device)
    optimizer.zero_grad()
    # 1. Forward pass (model outputs raw logits )
    out = model(x)
    # 2. Calculate loss (per batches)
    loss = criterion(out, y)
    # 3. Perform backpropogation
    loss.backward()
    # 4. Perform Gradient Descent
    optimizer.step()

    total_loss += loss.item() * x.size(0)
    correct += (out.argmax(1) == y).sum().item()
  return total_loss / len(loader.dataset), correct / len(loader.dataset)


def evaluate(model, loader):
  model.eval()
  correct = 0
  with torch.inference_mode():
    for x, y in loader:
      x, y = x.to(device), y.to(device)
      out = model(x)
      correct += (out.argmax(dim=1) == y).sum().item()
  return correct / len(loader.dataset)

from tqdm.auto import tqdm

# Training

train_accuracies, test_accuracies = [], []

for epoch in tqdm(range(EPOCHS)):
  # Pass criterion and optimizer in the correct order
  train_loss, train_acc = train(model, train_loader, criterion, optimizer)
  test_acc = evaluate(model, test_loader)
  train_accuracies.append(train_acc)
  test_accuracies.append(test_acc)
  print(f"Epoch: {epoch+1}/{EPOCHS}, Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}%, Test acc: {test_acc:.4f}")

train_accuracies

test_accuracies

plt.plot(train_accuracies, label="Train Accuracy")
plt.plot(test_accuracies, label="Test Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Training and Test Accuracy")
plt.show()

import random

len(test_dataset)

test_dataset[0][0].unsqueeze(dim=0).shape

test_dataset[0][0]/2+0.5

def predict_and_plot_grid(model,
                          dataset,
                          classes,
                          grid_size=3):
    model.eval()
    fig, axes = plt.subplots(grid_size, grid_size, figsize=(9, 9))
    for i in range(grid_size):
        for j in range(grid_size):
            idx = random.randint(0, len(dataset) - 1)
            img, true_label = dataset[idx]
            input_tensor = img.unsqueeze(dim=0).to(device)
            with torch.inference_mode():
                output = model(input_tensor)
                _, predicted = torch.max(output.data, 1)
            img = img / 2 + 0.5
            npimg = img.cpu().numpy()
            axes[i, j].imshow(np.transpose(npimg, (1, 2, 0)))
            truth = classes[true_label] == classes[predicted.item()]
            if truth:
                color = "g"
            else:
                color = "r"

            axes[i, j].set_title(f"Truth: {classes[true_label]}\n, Predicted: {classes[predicted.item()]}", fontsize=10, c=color)
            axes[i, j].axis("off")
    plt.tight_layout()
    plt.show()

predict_and_plot_grid(model=model,
                      dataset=test_dataset,
                      classes=train_dataset.classes,
                      grid_size=3)

